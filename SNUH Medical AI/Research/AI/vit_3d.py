import math
from functools import partial
from itertools import repeat
from typing import Callable, List, Optional, Sequence, Tuple, Union
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from timm.models.vision_transformer import Block

try:
    from torch import _assert
except ImportError:
    def _assert(condition: bool, message: str):
        assert condition, message

import torch
import math
import warnings

class PropHazards(nn.Module):
  def __init__(self, size_in, size_out):#, device):
    super().__init__()
    self.linear = nn.Linear(size_in, size_out)#.to(device)

  def forward(self, x):
    x = self.linear(x)
    
    return torch.pow(torch.sigmoid(x), torch.exp(x)) #.float().to(device)
  
class ClsExtractor(nn.Module):
  
  def __init__(self,model):
      super().__init__()
      self.patch_embed = model.patch_embed
      self.cls_token = model.cls_token
      self.pos_drop = model.pos_drop
      self.pos_embed = model.pos_embed
      self.blocks = model.blocks
      self.cls_norm = model.cls_norm
  
  def forward_features(self, x, img_con=None):
    B = x.shape[0]
    x = self.patch_embed(x)
    data_len = x.shape[1]
    # print(f'Embedding Patch Sequence:{data_len}')

    cls_tokens = self.cls_token.expand(B, -1, -1)
    x = torch.cat((cls_tokens, x), dim=1)
    x = x + self.pos_embed
    # print(f'Positional Embedding Vector Size():{self.pos_embed.size()}')
    x = self.pos_drop(x)

    for blk in self.blocks:
        x = blk(x)
    cls = self.cls_norm(x[:, 0]) # torch.size([16,1024])

    return cls


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


class PatchEmbed(nn.Module):
    """ 3D Image to Patch Embedding
    """

    def __init__(
            self,
            img_size: Optional[int] = 224,
            patch_size: int = 16,
            in_chans: int = 4,
            embed_dim: int = 768,
            norm_layer: Optional[Callable] = None,
            flatten: bool = True,
            bias: bool = True,
            strict_img_size: bool = True,
    ):
        super().__init__()
        self.patch_size = tuple(repeat(patch_size, 4)) if isinstance(patch_size, int) else patch_size
        
        if img_size is not None:
            self.img_size = tuple(repeat(img_size, 4)) if isinstance(img_size, int) else img_size
            self.grid_size = tuple([s // p for s, p in zip(self.img_size, self.patch_size)])
            self.num_patches = self.grid_size[0] * self.grid_size[1] * self.grid_size[2]
        else:
            self.img_size = None
            self.grid_size = None
            self.num_patches = None

        self.flatten = flatten
        self.strict_img_size = strict_img_size

        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)
        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()

    def forward(self, x):
        B, C, H, W, D = x.shape
        if self.img_size is not None:
            if self.strict_img_size:
                _assert(H == self.img_size[0], f"Input height ({H}) doesn't match model ({self.img_size[0]}).")
                _assert(W == self.img_size[1], f"Input width ({W}) doesn't match model ({self.img_size[1]}).")
                _assert(D == self.img_size[2], f"Input width ({D}) doesn't match model ({self.img_size[2]}).")
            else:
                _assert(
                    H % self.patch_size[0] == 0,
                    f"Input height ({H}) should be divisible by patch size ({self.patch_size[0]})."
                )
                _assert(
                    W % self.patch_size[1] == 0,
                    f"Input width ({W}) should be divisible by patch size ({self.patch_size[1]})."
                )
                _assert(
                    D % self.patch_size[2] == 0,
                    f"Input width ({D}) should be divisible by patch size ({self.patch_size[2]})."
                )
        
        # print(f'3D MRI GBM Size:{x.size()}')
        x = self.proj(x)
        # print(f'Batch,Embed_dim,patch_num:{x.size()}')
        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC
        # print(f'After Patch Embedding:{x.size()}')
        x = self.norm(x)
        return x


class VisionTransformer(nn.Module):
    """ Vision Transformer with support for patch or hybrid CNN input stage
    """
    def __init__(self, args, img_size=224, patch_size=16, in_chans=1, num_classes=1024, embed_dim=768, depth=12,
                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,
                 drop_path_rate=0., global_pool=False, hybrid_backbone=None, norm_layer=nn.LayerNorm):
        super().__init__()
        self.num_classes = num_classes
        self.args = args
        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
        self.prohazards = PropHazards(self.embed_dim,args.n_intervals)

        # --------------------------------------------------------------------------
        # ViT encoder specifics
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1,embed_dim))
        self.pos_drop = nn.Dropout(p=drop_rate)
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        self.blocks = nn.ModuleList([
            Block(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
                proj_drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)
            for i in range(depth)])
        self.cls_norm = norm_layer(embed_dim)
        # --------------------------------------------------------------------------

        # Classifier head
        # self.heads = nn.ModuleList([nn.Linear(embed_dim, 1) for _ in range(num_classes)]) if num_classes > 0 else nn.Identity()
        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()

        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here
        #self.repr = nn.Linear(embed_dim, representation_size)
        #self.repr_act = nn.Tanh()
        trunc_normal_(self.pos_embed, std=.02)
        trunc_normal_(self.cls_token, std=.02)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed', 'cls_token', 'CA_token'}

    def get_classifier(self):
        return self.head

    def reset_classifier(self, num_classes, global_pool=''):
        self.num_classes = num_classes
        self.heads = nn.ModuleList([nn.Linear(self.embed_dim, 1) for _ in range(num_classes)]) if num_classes > 0 else nn.Identity()

    def forward_features(self, x, img_con=None):
        B = x.shape[0]
        x = self.patch_embed(x)
        data_len = x.shape[1]
        # print(f'Embedding Patch Sequence:{data_len}')

        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks
        x = torch.cat((cls_tokens, x), dim=1)
        x = x + self.pos_embed
        # print(f'Positional Embedding Vector Size():{self.pos_embed.size()}')
        x = self.pos_drop(x)

        for blk in self.blocks:
            x = blk(x)

        cls = self.cls_norm(x[:, 0]) # torch.size([16,1024])

        return cls

    def forward(self, x, img_con=None):
        cls = self.forward_features(x, img_con)
        # print(f'Before MLP Head cls.size:{cls.size()}') # torch.size(batch_size,embed_dim)
        # x = torch.cat([head(x) for head in self.heads], dim=1)

        if self.num_classes == 5:
            x = self.head(cls).squeeze(1) # loss -> TaylorCrossEntropyloss (Softmax + CrossEntropy + LabelSmoothing)
        
        elif self.num_classes == self.args.n_intervals:
            x = self.prohazards(cls).squeeze(1)
        
        return x

def vit_glioma_type_classifier(**kwargs):
    model = VisionTransformer(
        patch_size=32, in_chans=4, num_classes=5, embed_dim=1024, depth=24, num_heads=16, mlp_ratio = 4, qkv_bias = True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),**kwargs)
    return model

def vit_gbm_patch32(**kwargs):
    model = VisionTransformer(
        patch_size=32,in_chans=4,embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),**kwargs)
    return model

def vit_small_patch16(**kwargs):
    model = VisionTransformer(
        embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


def vit_base_patch16(**kwargs):
    model = VisionTransformer(
        embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


def vit_large_patch16(**kwargs):
    model = VisionTransformer(
        embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


def vit_huge_patch14(**kwargs):
    model = VisionTransformer(
        embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model

